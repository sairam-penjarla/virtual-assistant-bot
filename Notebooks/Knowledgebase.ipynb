{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading Websites, PDFs and Saving Data to .txt Files\n",
    "\n",
    "In this section, we extract the content from the provided URLs (webpages) and save the text data into `.txt` files. This is done by scraping the content from the web pages, removing unnecessary formatting, and storing the clean text for further processing.\n",
    "\n",
    "**Key steps:**\n",
    "- Identify the URLs for webpages.\n",
    "- Scrape the webpage content.\n",
    "- Clean and process the content.\n",
    "- Save the processed text into `.txt` files for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Ensure dataset directory exists\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "# Combined dictionary mapping URLs to filenames (both webpages and PDFs)\n",
    "url_filename_mapping = {\n",
    "    \"https://www.larsentoubro.com/corporate/about-lt-group/overview/\": \"Larsen_Toubro_Overview.txt\",\n",
    "    \"https://www.larsentoubro.com/corporate/about-lt-group/technology-for-growth/\": \"Larsen_Toubro_Technology_for_Growth.txt\",\n",
    "    \"https://www.larsentoubro.com/corporate/about-lt-group/awards-recognition/\": \"Larsen_Toubro_Awards_Recognition.txt\",\n",
    "    \"https://www.larsentoubro.com/corporate/about-lt-group/leadership/\": \"Larsen_Toubro_Leadership.txt\",\n",
    "    \"https://www.larsentoubro.com/corporate/about-lt-group/facilities/\": \"Larsen_Toubro_Facilities.txt\",\n",
    "    \"https://www.larsentoubro.com/corporate/about-lt-group/experience-centre-mumbai/\": \"Larsen_Toubro_Experience_Centre_Mumbai.txt\",\n",
    "    \"https://www.lntsustainability.com/overview/\": \"LNT_Sustainability_Overview.txt\",\n",
    "    \"https://www.lntsustainability.com/climate-strategy/\": \"LNT_Climate_Strategy.txt\",\n",
    "    \"https://www.lntsustainability.com/environment/\": \"LNT_Environment.txt\",\n",
    "    \"https://www.lntsustainability.com/green-business/\": \"LNT_Green_Business.txt\",\n",
    "    \"https://www.larsentoubro.com/corporate/careers/learning-development/\": \"LNT_Careers_Learning_Development.txt\",\n",
    "    \"https://www.larsentoubro.com/corporate/careers/diversity-equity-inclusion/\": \"LNT_Careers_Diversity_Equity_Inclusion.txt\",\n",
    "    \"https://www.larsentoubro.com/corporate/careers/recruitment-caution/\": \"LNT_Careers_Recruitment_Caution.txt\",\n",
    "    \"https://www.larsentoubro.com/corporate/careers/campus-recruitment/\": \"LNT_Careers_Campus_Recruitment.txt\",\n",
    "    \"https://www.larsentoubro.com/corporate/careers/renew-career-re-entry-for-women/\": \"LNT_Careers_ReEntry_for_Women.txt\",\n",
    "    \n",
    "    # PDF URLs (Added these to the same dictionary)\n",
    "    \"https://annualreview.larsentoubro.com/download/L&T-Annual-Review-2024.pdf\": \"LT_Annual_Review_2024.pdf\",\n",
    "    \"https://annualreview.larsentoubro.com/download/L&T%20Annual%20Review%202023.pdf\": \"LT_Annual_Review_2023.pdf\",\n",
    "    \"https://annualreview.larsentoubro.com/download/Annual%20Review%202022.pdf\": \"Annual_Review_2022.pdf\",\n",
    "    \"https://annualreview.larsentoubro.com/download/Annual_Review_2021.pdf\": \"Annual_Review_2021.pdf\",\n",
    "    \"https://annualreview.larsentoubro.com/download/L&T%20Annual%20Review%202020.pdf\": \"LT_Annual_Review_2020.pdf\"\n",
    "}\n",
    "\n",
    "# Ensure dataset directory exists\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "def save_webpages_as_text(urls):\n",
    "    for url, filename in urls.items():\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            text = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "            with open(f\"dataset/{filename}\", \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(text)\n",
    "\n",
    "            print(f\"Saved: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "def download_and_parse_pdfs(pdf_urls):\n",
    "    for url, filename in pdf_urls.items():\n",
    "        pdf_filepath = f\"dataset/{filename}\"  # Save inside dataset folder\n",
    "        txt_filename = filename.replace(\".pdf\", \".txt\")  # Convert to text file name\n",
    "        txt_filepath = f\"dataset/{txt_filename}\"\n",
    "\n",
    "        try:\n",
    "            # Step 1: Download the PDF\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with open(pdf_filepath, \"wb\") as file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    file.write(chunk)\n",
    "\n",
    "            print(f\"Downloaded: {pdf_filepath}\")\n",
    "\n",
    "            # Step 2: Extract text from the PDF\n",
    "            with open(pdf_filepath, \"rb\") as pdf_file:\n",
    "                reader = PdfReader(pdf_file)\n",
    "                extracted_text = \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "\n",
    "            # Step 3: Save extracted text to a .txt file\n",
    "            with open(txt_filepath, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "                txt_file.write(extracted_text)\n",
    "\n",
    "            print(f\"Extracted text saved: {txt_filepath}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "def process_text_files(urls):\n",
    "    \"\"\"\n",
    "    Processes all URLs, sending PDFs to download_and_parse_pdfs and webpages to save_webpages_as_text.\n",
    "    \"\"\"\n",
    "    pdf_urls = {url: filename for url, filename in urls.items() if url.endswith('.pdf')}\n",
    "    webpage_urls = {url: filename for url, filename in urls.items() if not url.endswith('.pdf')}\n",
    "\n",
    "    # Process PDFs\n",
    "    if pdf_urls:\n",
    "        download_and_parse_pdfs(pdf_urls)\n",
    "\n",
    "    # Process Webpages\n",
    "    if webpage_urls:\n",
    "        save_webpages_as_text(webpage_urls)\n",
    "\n",
    "# Run the function to process all URLs (PDFs and Webpages)\n",
    "process_text_files(url_filename_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Chunk Data and Create JSON Files\n",
    "\n",
    "Once we have the raw text data from the `.txt` files, the next step is to split it into smaller, manageable chunks. This is important to keep the context intact while making the data easier to handle for later steps. After chunking the data, we store the chunks along with metadata (like the filename and URL) in a JSON file.\n",
    "\n",
    "**Key steps:**\n",
    "- Read the text data from `.txt` files.\n",
    "- Split the data into smaller chunks using a recursive text splitter.\n",
    "- Save each chunk with metadata in a single JSON file for easy access and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, filename, url):\n",
    "    \"\"\"\n",
    "    Preprocess the text by:\n",
    "    1. Converting to lowercase.\n",
    "    2. Removing excessive newlines (\\n), keeping max 2 consecutive.\n",
    "    3. Splitting into smaller chunks using Recursive Text Splitter.\n",
    "    4. Storing chunks in the global list with metadata.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Limit newlines to max 2 consecutive\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Adjust chunk size as needed\n",
    "        chunk_overlap=100  # Overlapping to maintain context\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    chunks_data = []\n",
    "\n",
    "    # Append data to the global list\n",
    "    for chunk in chunks:\n",
    "        chunks_data.append({\n",
    "            \"chunk_data\": chunk,\n",
    "            \"metadata\": {\n",
    "                \"filename\": filename,\n",
    "                \"url\": url\n",
    "            }\n",
    "        })\n",
    "\n",
    "    print(f\"Processed and stored chunks for {filename}.\")\n",
    "    return chunks_data\n",
    "\n",
    "def chunk_data(urls):\n",
    "    # Global list to store all chunks data\n",
    "    all_chunks_data = []\n",
    "    # Process text files for chunking and JSON storage\n",
    "    for url, filename in urls.items():\n",
    "        if filename.endswith('.txt'):\n",
    "            try:\n",
    "                with open(f\"dataset/{filename}\", \"r\", encoding=\"utf-8\") as file:\n",
    "                    text = file.read()\n",
    "                chunks_data = preprocess_text(text, filename, url)\n",
    "                all_chunks_data += chunks_data\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    # Once all files are processed, save all chunks to a single JSON file\n",
    "    with open(\"dataset/all_chunks_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(all_chunks_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"All chunks processed and saved to 'all_chunks_data.json'.\")\n",
    "    \n",
    "# Run the function to chunk all data\n",
    "chunk_data(url_filename_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Embeddings\n",
    "\n",
    "Embeddings are a way to represent textual data in a numerical format that machine learning models can process. Here, we use a pre-trained model from `sentence-transformers` to generate embeddings for each of the chunks. These embeddings capture the semantic meaning of the text and can be used for similarity comparisons.\n",
    "\n",
    "**Key steps:**\n",
    "- Use the `sentence-transformers` library to load a pre-trained model.\n",
    "- Convert each chunk of text into an embedding.\n",
    "- Store the embeddings for further processing and querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Take Chunks and Convert Them to Embeddings and Store in Collection\n",
    "\n",
    "This step involves taking the chunked data, converting each chunk into an embedding using the `sentence-transformers` model, and then storing these embeddings in a collection for efficient similarity search. We use a tool like `chromadb` to store the embeddings in a database-like structure that supports fast similarity lookups.\n",
    "\n",
    "**Key steps:**\n",
    "- Convert each chunk of text into embeddings using the pre-trained model.\n",
    "- Store the embeddings in a collection (e.g., `chromadb`) for efficient retrieval.\n",
    "- Ensure the collection is indexed and ready for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize Chroma client and create a collection\n",
    "client = chromadb.PersistentClient(path=\"../knowledge_base\")\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"embeddings_collection\")\n",
    "\n",
    "def create_embeddings_from_json(json_filename):\n",
    "    \"\"\"\n",
    "    Read the JSON file containing chunk data, generate embeddings using SentenceTransformer,\n",
    "    and create a chroma collection.\n",
    "    \"\"\"\n",
    "    # Load the chunk data from the JSON file\n",
    "    with open(json_filename, 'r', encoding='utf-8') as file:\n",
    "        chunk_data = json.load(file)\n",
    "\n",
    "    # Prepare sentences (chunks) and metadata for embedding\n",
    "    sentences = [chunk['chunk_data'] for chunk in chunk_data]\n",
    "    metadatas = [chunk['metadata'] for chunk in chunk_data]\n",
    "\n",
    "    # Generate embeddings using the SentenceTransformer model\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    # Insert the embeddings and metadata into the Chroma collection\n",
    "    for idx, embedding in enumerate(embeddings):\n",
    "        # You can optionally store metadata alongside each embedding\n",
    "        collection.add(\n",
    "            ids=[str(idx)],  # Unique ID for each chunk\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[metadatas[idx]],  # Store metadata (filename, URL)\n",
    "            documents=[sentences[idx]]  # Store the sentence (chunk)\n",
    "        )\n",
    "\n",
    "    print(f\"Created embeddings for {len(sentences)} chunks and added to Chroma collection.\")\n",
    "\n",
    "# Run the function to create embeddings and add to Chroma collection\n",
    "create_embeddings_from_json('dataset/all_chunks_data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_documents = len(collection.get()['documents'])\n",
    "\n",
    "print(f\"Number of documents in the collection: {num_documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Query and See the Top 10 Results\n",
    "\n",
    "Now that the data is stored as embeddings in a collection, we can query the collection using a query string. The system will return the top 10 most relevant results based on semantic similarity to the query. This is useful for finding the most relevant documents or text chunks based on a user’s input.\n",
    "\n",
    "**Key steps:**\n",
    "- Take a user query and convert it into an embedding.\n",
    "- Search the collection for the most similar embeddings.\n",
    "- Retrieve and display the top 10 relevant results, showing their similarity scores and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fetch_top_relevant_queries(query, collection, top_k=10):\n",
    "    \"\"\"\n",
    "    Takes a query, encodes it using the SentenceTransformer, and fetches the top `top_k` relevant queries from the Chroma collection.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the query into an embedding\n",
    "    query_embedding = model.encode([query])\n",
    "\n",
    "    # Perform a similarity search in the Chroma collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,  # The query embedding\n",
    "        n_results=top_k  # Number of top results to return\n",
    "    )\n",
    "\n",
    "    # Process the results\n",
    "    relevant_queries = []\n",
    "    for result in results['documents']:\n",
    "        relevant_queries.append({\n",
    "            \"document\": result,  # The chunk or sentence text\n",
    "            \"metadata\": results['metadatas'][results['documents'].index(result)],  # Metadata for each chunk\n",
    "            \"score\": results['distances'][results['documents'].index(result)]  # Similarity score (distance)\n",
    "        })\n",
    "\n",
    "    return relevant_queries\n",
    "\n",
    "\n",
    "query = \"Who are the founders\"\n",
    "top_queries = fetch_top_relevant_queries(query, collection, top_k=10)\n",
    "\n",
    "# Display the top 10 relevant queries\n",
    "for i in range(len(top_queries[0]['metadata'])):\n",
    "    print(\"Document: \" + str(top_queries[0]['document'][i].replace(\"\\n\", \" \")))\n",
    "    print(\"Metadata: \" + str(top_queries[0]['metadata'][i]))\n",
    "    print(\"Similarity Score: \" + str(top_queries[0]['score'][i]))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. How to Use OpenAI with System and User Prompts  \n",
    "\n",
    "### Description  \n",
    "In this section, we explore how to interact with OpenAI models using **system and user prompts**. The **system prompt** sets the AI's behavior and context, while the **user prompt** provides the input for generating responses. We demonstrate how to structure these messages effectively for improved outputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "TOKEN = os.environ.get('TOKEN')\n",
    "HOST = os.environ.get('HOST')\n",
    "MODEL = os.environ.get('MODEL')\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key = TOKEN,\n",
    "  base_url = HOST\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "  messages=[\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are an AI assistant\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Tell me about Large Language Models\"\n",
    "  }\n",
    "  ],\n",
    "  model=MODEL,\n",
    "  max_tokens=256\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. How to Create Augmented Chunks  \n",
    "\n",
    "### Description  \n",
    "Augmented chunks refer to **enhanced text segments** that contain additional context, metadata, or summaries to improve retrieval and response generation. This section covers methods to **generate meaningful chunks** from text, **enrich them with metadata**, and **store them efficiently** for downstream tasks like document retrieval and question-answering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "TOKEN = os.environ.get('TOKEN')\n",
    "HOST = os.environ.get('HOST')\n",
    "MODEL = os.environ.get('MODEL')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "  api_key=TOKEN,\n",
    "  base_url=f\"{HOST}/serving-endpoints\"\n",
    ")\n",
    "\n",
    "def get_random_chunk_and_generate_questions(json_file_path):\n",
    "    \"\"\"\n",
    "    Selects a random chunk from the provided JSON file and sends it to OpenAI to generate a list of questions\n",
    "    that can be answered using the chunk of text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the JSON file and load the chunks data\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "            chunks_data = json.load(json_file)\n",
    "\n",
    "        random_chunk = random.choice(chunks_data)\n",
    "        \n",
    "        random_chunk_data = random_chunk['chunk_data']\n",
    "\n",
    "        # Prepare the prompt for OpenAI\n",
    "        prompt = f\"Please generate a set of questions that could be answered using this information: \\n\\n{random_chunk_data}\\n\\n\"\n",
    "\n",
    "        # Send the prompt to OpenAI\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant with the ability to generate relevant questions based on provided text. \"\n",
    "                            \"Your task is to analyze the text and create insightful questions that can be answered using that text.\"\n",
    "                            \"return only the questions in plain text in multiple lines. no headings, no titles, nothing, no bulletpoints\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=MODEL,\n",
    "        )\n",
    "\n",
    "        # Get the response and print the generated questions\n",
    "        questions = chat_completion.choices[0].message.content\n",
    "        \n",
    "        return random_chunk, questions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Example usage\n",
    "chunk, llm_output = get_random_chunk_and_generate_questions('dataset/all_chunks_data.json')\n",
    "\n",
    "\n",
    "print(chunk)\n",
    "print(\"\\n\\n\\n\")\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = llm_output.split(\"\\n\")\n",
    "\n",
    "qes = questions[0]\n",
    "augumented_chunk = f\"{qes}{chunk['chunk_data']}\"\n",
    "print(augumented_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augumented_chunks = []\n",
    "for question in questions:\n",
    "    chunk = {\n",
    "        \"chunk_data\": f\"{question}\\n\\n{chunk['chunk_data']}\",\n",
    "        \"metadata\": chunk['metadata']\n",
    "    }\n",
    "    chunk['metadata']['is_augumented'] = True\n",
    "    augumented_chunks.append(chunk)\n",
    "\n",
    "augumented_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(augumented_chunks)\n",
    "print(\"Number of augumented chunks: \", len(augumented_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def add_augumented_chunks_to_collection(augumented_chunks):\n",
    "    # Prepare sentences (chunks) and metadata for embedding\n",
    "    sentences = [chunk['chunk_data'] for chunk in augumented_chunks]\n",
    "    metadatas = [chunk['metadata'] for chunk in augumented_chunks]\n",
    "\n",
    "    # Generate embeddings using the SentenceTransformer model\n",
    "    embeddings = model.encode(sentences)\n",
    "    last_idx = len(collection.get()['documents'])\n",
    "    # Insert the embeddings and metadata into the Chroma collection\n",
    "    for idx, embedding in enumerate(embeddings):\n",
    "        # You can optionally store metadata alongside each embedding\n",
    "        \n",
    "        collection.add(\n",
    "            ids=[str(idx+last_idx)],  # Unique ID for each chunk\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[metadatas[idx]],  # Store metadata (filename, URL)\n",
    "            documents=[sentences[idx]]  # Store the sentence (chunk)\n",
    "        )\n",
    "\n",
    "    print(f\"Created embeddings for {len(sentences)} chunks and added to Chroma collection.\")\n",
    "\n",
    "# Run the function to create embeddings and add to Chroma collection\n",
    "add_augumented_chunks_to_collection(augumented_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. How to Perform Re-Ranking  \n",
    "\n",
    "### Description  \n",
    "Re-ranking helps improve **search relevance** by refining initial retrieval results using **semantic similarity scoring** or **cross-encoder models**. This section explains how to use techniques like **CrossEncoders** to **re-score and sort** retrieved documents, ensuring that the most relevant results appear at the top. We explore practical implementations using **sentence transformers** for better ranking and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is the chairman of L&T?\"\n",
    "top_queries = fetch_top_relevant_queries(query, collection, top_k=10)\n",
    "\n",
    "# Display the top 10 relevant queries\n",
    "for i in range(len(top_queries[0]['metadata'])):\n",
    "    print(\"Document: \" + str(top_queries[0]['document'][i].replace(\"\\n\", \" \")))\n",
    "    print(\"Metadata: \" + str(top_queries[0]['metadata'][i]))\n",
    "    print(\"Similarity Score: \" + str(top_queries[0]['score'][i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load the cross-encoder model\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "query = \"Who is the chairman of L&T?\"\n",
    "top_queries = fetch_top_relevant_queries(query, collection, top_k=10)\n",
    "\n",
    "# Compute cross-encoder scores\n",
    "for i in range(len(top_queries[0]['metadata'])):\n",
    "    document_text = str(top_queries[0]['document'][i].replace(\"\\n\", \" \"))\n",
    "    \n",
    "    # Compute cross-encoder relevance score\n",
    "    score = cross_encoder.predict([(query, document_text)])\n",
    "    \n",
    "    print(\"Document: \" + document_text)\n",
    "    print(\"Similarity Score: \" + str(top_queries[0]['score'][i]))\n",
    "    print(\"Cross-Encoder Score: \" + str(score[0]))  # Displaying the cross-encoder score\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load the cross-encoder model\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "query = \"Who is the chairman of L&T?\"\n",
    "top_queries = fetch_top_relevant_queries(query, collection, top_k=10)\n",
    "\n",
    "# Compute cross-encoder scores and store them with the query data\n",
    "scored_queries = []\n",
    "for i in range(len(top_queries[0]['metadata'])):\n",
    "    document_text = str(top_queries[0]['document'][i].replace(\"\\n\", \" \"))\n",
    "    \n",
    "    # Compute cross-encoder relevance score\n",
    "    score = cross_encoder.predict([(query, document_text)])[0]  \n",
    "    \n",
    "    # Append to list with all relevant data\n",
    "    scored_queries.append({\n",
    "        \"document\": document_text,\n",
    "        \"metadata\": top_queries[0]['metadata'][i],\n",
    "        \"similarity_score\": top_queries[0]['score'][i],\n",
    "        \"cross_encoder_score\": score\n",
    "    })\n",
    "\n",
    "# Sort the queries based on cross-encoder score in descending order\n",
    "scored_queries.sort(key=lambda x: x['cross_encoder_score'], reverse=True)\n",
    "\n",
    "# Display sorted results\n",
    "for item in scored_queries:\n",
    "    print(\"Document: \" + item[\"document\"])\n",
    "    print(\"Metadata: \" + str(item[\"metadata\"]))\n",
    "    print(\"Similarity Score: \" + str(item[\"similarity_score\"]))\n",
    "    print(\"Cross-Encoder Score: \" + str(item[\"cross_encoder_score\"]))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "writer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
